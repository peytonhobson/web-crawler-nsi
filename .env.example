# API Keys
OPENAI_API_KEY=your_openai_api_key_here

# Pinecone Configuration
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_INDEX_NAME=your_index_name_here

# Crawler Configuration
# Comma-separated list of start URLs
START_URLS=https://example.com/page1,https://example.com/page2
# Maximum crawl depth (0 means only starting URLs)
MAX_DEPTH=2
# Number of URLs to process in each batch
BATCH_SIZE=10
# Comma-separated list of HTML tags to exclude from processing
EXCLUDED_TAGS=footer,nav,header

# LLM Settings
# OpenAI model provider to use
LLM_PROVIDER=openai/gpt-4o-mini
# Custom instruction for content filtering (if needed)
# LLM_INSTRUCTION=Your custom instruction here

# Chunking Configuration
# Size of text chunks for vector storage
CHUNK_SIZE=500
# Overlap between chunks to maintain context (as ratio)
CHUNK_OVERLAP_RATIO=0.2

# Summarization Configuration
# Model to use for summarizing and keyword generation
SUMMARY_MODEL_NAME=gpt-4o-mini
# Temperature for generation (lower is more deterministic)
SUMMARY_TEMPERATURE=0.3
# Number of worker threads for parallel processing
SUMMARY_MAX_WORKERS=10

# Vector DB Configuration
# Prefix for chunk IDs in vector database
CHUNK_ID_PREFIX=web_crawl
# How many hours to keep records before deletion
RECORD_RETENTION_HOURS=1
# Number of records to upsert in each batch
UPSERT_BATCH_SIZE=50
# Whether to delete old records
DELETE_OLD_RECORDS=true

# Directory Configuration
# Where processed content is stored
OUTPUT_DIR=cleaned_output
# Where logs are stored
LOGS_DIR=logs

# Processing Control
# Whether to save locally instead of uploading to Pinecone
DRY_RUN=false
# Whether to display verbose logs
VERBOSE=false